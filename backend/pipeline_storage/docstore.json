{"docstore/metadata": {"c523865b-0fd9-49c4-9145-fd6eda892a93": {"doc_hash": "b4187eb2bf7b1a42cbcf623b4487634e4a5f0fb8887260eab51c2ab90e278def"}, "18daed42-09e8-4be6-bbcb-fd1266d552db": {"doc_hash": "42b9c4980c00699ed0a5a0117b08380623b727609f4b7092a0f46f0c7868eaae"}, "05b44427-545f-4351-bc30-844a1b947d22": {"doc_hash": "075d6489898b5ee3dbd8812f52eb407a0b7074689c393ca3bf30e06057245c3c"}, "59cc2f21-53a4-401e-96c3-219c32cce9d7": {"doc_hash": "0e161a7bc2f2eb197169d437778e585bdbb9705424265439773407099f497c54"}}, "docstore/data": {"c523865b-0fd9-49c4-9145-fd6eda892a93": {"__data__": {"id_": "c523865b-0fd9-49c4-9145-fd6eda892a93", "embedding": null, "metadata": {"filename": "concepts.md", "extension": ".md", "file_path": "getting_started/concepts", "file_name": "concepts.md", "file_type": "text/markdown", "file_size": 3688, "creation_date": "2024-03-08", "last_modified_date": "2024-03-02", "summary": "Learn how LlamaIndex.TS facilitates building LLM-powered applications by guiding you through the indexing and querying stages. Discover data loaders, documents, indexes, retrievers, and response synthesizers. Explore how to create query engines and chat engines for Q&A and chatbot applications, enabling efficient retrieval and synthesis of context for accurate responses."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 3\n---\n\n# Concepts\n\nLlamaIndex.TS helps you build LLM-powered applications (e.g. Q&A, chatbot) over custom data.\n\nIn this high-level concepts guide, you will learn:\n\n- how an LLM can answer questions using your own data.\n- key concepts and modules in LlamaIndex.TS for composing your own query pipeline.\n\n## Answering Questions Across Your Data\n\nLlamaIndex uses a two stage method when using an LLM with your data:\n\n1. **indexing stage**: preparing a knowledge base, and\n2. **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n\n!\n\nThis process is also known as Retrieval Augmented Generation (RAG).\n\nLlamaIndex.TS provides the essential toolkit for making both steps super easy.\n\nLet's explore each stage in detail.\n\n### Indexing Stage\n\nLlamaIndex.TS help you prepare the knowledge base with a suite of data connectors and indexes.\n\n!\n\n**Data Loaders**:\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n**Documents / Nodes**: A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. It's a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\n\n**Data Indexes**:\nOnce you've ingested your data, LlamaIndex helps you index data into a format that's easy to retrieve.\n\nUnder the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and stores your data in-memory or to disk.\n\n### Querying Stage\n\nIn the querying stage, the query pipeline retrieves the most relevant context given a user query,\nand pass that to the LLM (along with the query) to synthesize a response.\n\nThis gives the LLM up-to-date knowledge that is not in its original training data,\n(also reducing hallucination).\n\nThe key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.\n\nLlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent.\n\nThese building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.\n\n!\n\n#### Building Blocks\n\n**Retrievers**:\nA retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query.\nThe specific retrieval logic differs for difference indices, the most popular being dense retrieval against a vector index.\n\n**Response Synthesizers**:\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n\n#### Pipelines\n\n**Query Engines**:\nA query engine is an end-to-end pipeline that allow you to ask question over your data.\nIt takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n\n**Chat Engines**:\nA chat engine is an end-to-end pipeline for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "18daed42-09e8-4be6-bbcb-fd1266d552db": {"__data__": {"id_": "18daed42-09e8-4be6-bbcb-fd1266d552db", "embedding": null, "metadata": {"filename": "environments.md", "extension": ".md", "file_path": "getting_started/environments", "file_name": "environments.md", "file_type": "text/markdown", "file_size": 296, "creation_date": "2024-03-08", "last_modified_date": "2024-03-02", "summary": "LlamaIndex documentation outlines support for NodeJS 18 and 20. It specifies using NodeJS mode for NextJS App Router route handlers. The runtime default is set to \"nodejs\" for this purpose."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 2\n---\n\n# Environments\n\nLlamaIndex currently officially supports NodeJS 18 and NodeJS 20.\n\n## NextJS App Router\n\nIf you're using NextJS App Router route handlers/serverless functions, you'll need to use the NodeJS mode:\n\n```js\nexport const runtime = \"nodejs\"; // default\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "05b44427-545f-4351-bc30-844a1b947d22": {"__data__": {"id_": "05b44427-545f-4351-bc30-844a1b947d22", "embedding": null, "metadata": {"filename": "installation.mdx", "extension": ".mdx", "file_path": "getting_started/installation", "file_name": "installation.mdx", "file_size": 1240, "creation_date": "2024-03-08", "last_modified_date": "2024-03-02", "summary": "The LlamaIndex documentation page provides instructions for installing and setting up the LlamaIndex application using `create-llama` or NPM. It includes details on starting a development server, installing from NPM, and setting up environment variables like the OpenAI key. It emphasizes caution against checking in sensitive keys into version control."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 0\n---\n\n# Installation and Setup\n\nMake sure you have NodeJS v18 or higher.\n\n## Using create-llama\n\nThe easiest way to get started with LlamaIndex is by using `create-llama`. This CLI tool enables you to quickly start building a new LlamaIndex application, with everything set up for you.\n\nJust run\n\n<Tabs>\n<TabItem value=\"1\" label=\"npm\" default>\n\n```bash\nnpx create-llama@latest\n```\n\n</TabItem>\n<TabItem value=\"2\" label=\"Yarn\">\n\n```bash\nyarn create llama\n```\n\n</TabItem>\n<TabItem value=\"3\" label=\"pnpm\">\n\n```bash\npnpm create llama@latest\n```\n\n</TabItem>\n</Tabs>\n\nto get started. Once your app is generated, run\n\n```bash npm2yarn\nnpm run dev\n```\n\nto start the development server. You can then visit http://localhost:3000 to see your app\n\n## Installation from NPM\n\n```bash npm2yarn\nnpm install llamaindex\n```\n\n### Environment variables\n\nOur examples use OpenAI by default. You'll need to set up your Open AI key like so:\n\n```bash\nexport OPENAI_API_KEY=\"sk-......\" # Replace with your key from https://platform.openai.com/account/api-keys\n```\n\nIf you want to have it automatically loaded every time, add it to your `.zshrc/.bashrc`.\n\nWARNING: do not check in your OpenAI key into version control.\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "59cc2f21-53a4-401e-96c3-219c32cce9d7": {"__data__": {"id_": "59cc2f21-53a4-401e-96c3-219c32cce9d7", "embedding": null, "metadata": {"filename": "starter.mdx", "extension": ".mdx", "file_path": "getting_started/starter", "file_name": "starter.mdx", "file_size": 1261, "creation_date": "2024-03-08", "last_modified_date": "2024-03-02", "summary": "The LlamaIndex documentation guides users on setting up LlamaIndex.TS with an OpenAI key. It provides instructions for creating a project from scratch using node.js + TypeScript or Next.js + TypeScript. Users learn to install dependencies, write code to load data, create a document, index it with embeddings, and set up a query engine."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport CodeSource from \"!raw-loader!../../../../examples/vectorIndex\";\nimport TSConfigSource from \"!!raw-loader!../../../../examples/tsconfig.json\";\n\n# Starter Tutorial\n\nMake sure you have installed LlamaIndex.TS and have an OpenAI key. If you haven't, check out the installation guide.\n\n## From scratch(node.js + TypeScript):\n\nIn a new folder:\n\n```bash npm2yarn\nnpm init\nnpm install -D typescript @types/node\n```\n\nCreate the file `example.ts`. This code will load some example data, create a document, index it (which creates embeddings using OpenAI), and then creates query engine to answer questions about the data.\n\n<CodeBlock language=\"ts\">{CodeSource}</CodeBlock>\n\nCreate a `tsconfig.json` file in the same folder:\n\n<CodeBlock language=\"json\">{TSConfigSource}</CodeBlock>\n\nNow you can run the code with\n\n```bash\nnpx tsx example.ts\n```\n\nAlso, you can clone our examples and try them out:\n\n```bash npm2yarn\nnpx degit run-llama/LlamaIndexTS/examples my-new-project\ncd my-new-project\nnpm install\nnpx tsx ./vectorIndex.ts\n```\n\n## From scratch (Next.js + TypeScript):\n\nYou just need one command to create a new Next.js project:\n\n```bash npm2yarn\nnpx create-llama@latest\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}}}